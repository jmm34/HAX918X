\documentclass[notes,professionalfont,11pt,usenames,dvipsnames]{beamer}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage{pxfonts}
\usepackage{eulervm}

\usepackage{pgf,pgfarrows,pgfnodes,pgfautomata,pgfheaps,pgfshade}
\usepackage{tikz}
\usetikzlibrary{plothandlers,plotmarks,arrows,automata}
\usepackage{amsmath,amssymb}
\usepackage{colortbl}
\usepackage{mathrsfs}
\renewcommand{\mathcal}{\mathscr}
\usepackage{enumerate}

\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}

\newcommand{\disp}{\displaystyle}
\newcommand{\ds}{\displaystyle}
\newcommand{\vs}{\bigskip}
\newcommand{\esp}{\mathbb E}
\newcommand{\prob}{\mathbb P}
\newcommand{\ee}{\mathrm e}
\newcommand{\ii}{\mathrm i}
\newcommand{\dd}{\text{d}}
\newcommand{\Int}{\mathrm{int}\,}
\newcommand{\Cl}{\mathrm{adh}\,}
\newcommand{\Supp}{\operatorname{Supp}}
\newcommand{\Card}{\operatorname{Card}}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\Ave}{\operatorname{Ave}}
\renewcommand{\mathcal}{\mathscr}

\newcommand{\logit}{\operatorname{logit}}
\renewcommand{\epsilon}{\varepsilon}
\newcommand{\eps}{\varepsilon}
\newfont{\manfnt}{manfnt}
\newcommand{\danger}{{\manfnt\symbol{'177}}}
\newcommand{\mdanger}{\marginpar[\hfill\danger]{\danger\hfill}}
\newcommand{\new}{{\manfnt\symbol{30}}}
\newcommand{\mnew}{\marginpar[\hfill\new]{\hfill\new}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\tbrown}[1]{\textcolor{brown}{#1}}
\DeclareTextFontCommand{\hershey}{\fontfamily{hscs}\selectfont}
\newcommand{\tb}{\color{brown}}

\usetheme{Boadilla}
\usecolortheme{rose}

\newcommand\justify{\rightskip0pt \leftskip0pt}

%\usetheme{Goettingen}

\newenvironment{slide}
{\begin{frame}[environment=slide]
\frametitle{\insertsection \\ \insertsubsection}\justify\setlength{\parskip}{0.5cm}\vspace{-0.5cm}}
{\end{frame}}


\setbeamersize{text margin left=1cm}
\setbeamersize{text margin right=1cm}

%\setbeamersize{sidebar width left=1cm}
%\setbeamersize{sidebar width right=1cm}

\makeatletter
\newcommand{\setnextsection}[1]{%
  \setcounter{section}{\numexpr#1-1\relax}%
  \beamer@tocsectionnumber=\numexpr#1-1\relax\space}
\makeatother

\setbeamertemplate{itemize items}[triangle]
\setbeamercolor{itemize item}{fg=brown}
\setbeamertemplate{enumerate items}[circle]
\setbeamercolor{item projected}{bg=brown}
\setbeamercolor{block title}{fg=brown!100, bg=gray!20}
\setbeamercolor{block body}{bg=gray!10}

\title[MC and MCMC]{Monte Carlo and Markov chain Monte Carlo methods}
\author[Jean-Michel Marin]{Jean-Michel Marin}

\institute[IMAG]{University of Montpellier \\
Faculty of Sciences}

\date[HAX918X]{HAX918X / 2024-2025}

\begin{document}

\frame{\titlepage}

\frame{\tableofcontents} 

\section{Standard Monte Carlo method}

\begin{slide}

{\bf General definition} use of randomness to solve 
a problem centered on a calculation


There is no consensus to give a more precise definition


Methods that have been used for centuries: traces as far away as in 
Babylon and the Old Testament!

\end{slide}

\begin{slide}

\textbf{[1733, Buffon's Needle]} give an approximate value to $\pi$


Throw a $l$ long needle on a floor of parallel slats that create $d$ widths 
with $l\leq d$


If the needle is thrown uniformly on the ground (to be specified!), the probability that it 
intersects with one of the joins between the slats is $\displaystyle\frac{2l}{\pi d} $


If you make several independent rolls and you note $p$ the proportion of tests that hit 
one of the straight lines forming the separations between the slats, $\pi$ can be estimated by
$\displaystyle\frac{2l}{pd}$

\end{slide}

\begin{slide}

\textbf{[World War II, Los Alamos: Ulam, Metropolis and von Neumann]}
preparation of the first atomic bomb


The Monte Carlo appellation is due to Metropolis, inspired by Ulam's interest in poker


Work at Los Alamos: directly simulate neutron dispersion and absorption problems for fissile materials

\end{slide}

\begin{slide}

{\bf Theorem (strong law of large numbers)}
Let $(X_n)_{n\in\mathbb{N}}$ be an iid sequence of random variables with probability distribution $f$ \\
If $\E_f(|X_i|)<\infty$
$$
\bar X_n=\frac{1}{n}\sum_{i=1}^n X_i \longrightarrow_{\mbox{ps}} \E_f(X_1)
$$

\end{slide}

\begin{slide}

{\bf Theorem (central limit theorem)}
Let $(X_n)_{n\in\mathbb{N}}$ be an iid sequence of random variables with probability distribution $f$ \\
If $\E_f(|X_i|^2)<\infty$
$$
\sqrt{n}\left(\frac{\bar X_n - \E_f(X_1)}{\sqrt{\V_f(X_1)}}\right)\longrightarrow_{\mathcal{L}} N(0,1)
$$

\end{slide}

\begin{slide}

Target
$$
\E_f(h(X))=\int h(x)f(x)d\mu(x)<\infty
$$
($f$ is the density of $X$ with respect to $\mu$)


{\bf Standard Monte Carlo estimator of $\E_f(h(X))$
$$
\frac{1}{n}\sum_{i=1}^n h(X_i)
$$
where $X_1,\ldots,X_n$ is an iid sample from $f$}



\end{slide}

\begin{slide}

$$
\frac{1}{n}\sum_{i=1}^n h(X_i)\longrightarrow_{\mbox{ps}} \E_f(h(X))
$$


$$
\E_{f^{\otimes n}}\left(\frac{1}{n}\sum_{i=1}^n h(X_i)\right)=\E_f(h(X))
$$

\end{slide}

\begin{slide}

$$
\V_{f^{\otimes n}}\left[\frac{1}{n}\sum_{i=1}^nh(X_i)\right]=\frac{1}{n}\V_f(h(X))
$$


$$
\frac{1}{n}\left[\frac{1}{n-1}\sum_{i=1}^n\left(h(X_i)-\frac{1}{n}\sum_{j=1}^nh(X_j)\right)^2\right]
$$
is an unbiased estimator of $\V_f(h(X))\big/ n$

\end{slide}

\begin{slide}

If $\E_f(|h(X)|^2)<\infty$
$$
\frac{\sqrt{n}\left(\frac{1}{n}\sum_{i=1}^n h(X_i)-\E_f(h(X))\right)}{\sqrt{\V_f(h(X))}}
\longrightarrow_{\mathcal{L}} N(0,1)
$$

\end{slide}

\begin{slide}

Convergence speed for various quadrature rules and for the Monte Carlo method 
in $s$ dimension and using $n$ points


\begin{itemize}
\item Trapezoidal rule: $n^{-2/s}$
\item Simpson rule: $n^{-4/s}$
\item Gauss rule with $m$ points: $n^{-(2m-1)/s}$
\item Monte-Carlo method: $n^{-1/2}$
\end{itemize}

\end{slide}

\section{Importance Sampling methods}

\begin{slide}

Target
$$
\E_f(h(X))=\int h(x)f(x)d\mu(x)<\infty
$$


We consider the probability density $g$ (with respect to $\mu$)
such that: if $g(x)=0$ then $f(x)|h(x)|=0$

\end{slide}

\begin{slide}

$$
\E_f(h(X))=\int h(x)f(x)d\mu(x)=
$$
$$
\int h(x)\frac{f(x)}{g(x)}g(x)d\mu(x)=
\E_g\left[h(X)\frac{f(X)}{g(X)}\right]
$$


{\bf Importance sampling estimator of $\E_f(h(X))$
$$
\frac{1}{n}\sum_{i=1}^n h(X_i)\frac{f(X_i)}{g(X_i)}
$$
where $X_1,\ldots,X_n$ is an iid sample from $g$}

\end{slide}

\begin{slide}

\textbf{If $f|h|$ is absolutely continuous with respect to $g$}
$$
\frac{1}{n}\sum_{i=1}^n h(X_i)\frac{f(X_i)}{g(X_i)}\longrightarrow_{\mbox{ps}} \E_f(h(X))
$$
is convergent


$$
\E_{g^{\otimes n}}\left(\frac{1}{n}\sum_{i=1}^n h(X_i)\frac{f(X_i)}{g(X_i)}\right)=\E_f(h(X))
$$
is unbiased

\end{slide}

\begin{slide}

$$
\V_{g^{\otimes n}}\left[\frac{1}{n}\sum_{i=1}^n h(X_i)\frac{f(X_i)}{g(X_i)}\right]=\frac{1}{n}\V_g\left[h(X)\frac{f(X)}{g(X)}\right]
$$
where
$$
\V_g\left[h(X)\frac{f(X)}{g(X)}\right]=\E_f\left[h(X)^2\frac{f(X)}{g(X)}\right]-
\left[\E_f(h(X))\right]^2
$$ 


$$
\frac{1}{n}\left[\frac{1}{n-1}\sum_{i=1}^n\left(h(X_i)\frac{f(X_i)}{g(X_i)}-\frac{1}{n}\sum_{j=1}^nh(X_j)\frac{f(X_j)}{g(X_j)}\right)^2\right]
$$
is an unbiased estimator of $\V_g\left[h(X)\frac{f(X)}{g(X)}\right]\big/ n$

\end{slide}

\begin{slide}

The importance function that minimise $\ds \V_g\left[h(X)\frac{f(X)}{g(X)}\right]$ 
is
$$
g^*(x)=\frac{f(x)|h(x)|}{\int f(x)|h(x)|d\mu(x)}
$$

$f|h|$ is absolutely continuous with respect to $g^*$

\end{slide}

\begin{slide}

If $\E_g\left[\left|h(X)\frac{f(X)}{g(X)}\right|^2\right]=\E_f\left[\left|h(X)\right|^2\frac{f(X)}{g(X)}\right]<\infty$$$
\frac{\sqrt{n}\left(\frac{1}{n}\sum_{i=1}^n h(X_i)\frac{f(X_i)}{g(X_i)}-\E_f(h(X))\right)}{\sqrt{\V_g\left[h(X)f(X)\big/g(X)\right]}}
\longrightarrow_{\mathcal{L}} N(0,1)
$$


If $f(x)/g(x)<M$ and $\V_f(h(X))<\infty$
$$
\E_f\left[\left|h(X)\right|^2\frac{f(X)}{g(X)}\right]<\infty
$$

\end{slide}

\begin{slide}

There are many cases where the normalization constant of $f$ is unknown (Bayesian statistic)
$$
f(x)=\tilde f(x)\left/\int \tilde f(x) d\mu(x)=\tilde f(x)/c\right.
$$


{\bf Self-normalized importance sampling estimator of $\E_f(h(X))$ 
$$
\sum_{i=1}^n h(X_i)\frac{f(X_i)}{g(X_i)}\bigg/ \sum_{i=1}^n \frac{f(X_i)}{g(X_i)}
$$
where $X_1,\ldots,X_n$ is an iid sample from $g$}

\end{slide}

\begin{slide}

\textbf{If $f$ is absolutely continuous with respect to $g$},
$$
\sum_{i=1}^n h(X_i)\frac{f(X_i)}{g(X_i)}\bigg/\sum_{i=1}^n \frac{f(X_i)}{g(X_i)}\longrightarrow_{\mbox{ps}} \E_f(h(X))
$$
is convergent
 

$$
\E_{g^{\otimes n}}\left(\sum_{i=1}^n h(X_i)\frac{f(X_i)}{g(X_i)}\bigg/ \sum_{i=1}^n \frac{f(X_i)}{g(X_i)}\right)\neq \E_f(h(X))
$$

\end{slide}

\begin{slide}

If $\E_f\left[\left|h(X)\right|^2\frac{f(X)}{g(X)}\right]<\infty$, $\E_f\left[\frac{f(X)}{g(X)}\right]<\infty$,
$$
\sqrt{n}\left(\sum_{i=1}^n h(X_i)\frac{f(X_i)}{g(X_i)}\bigg/ \sum_{i=1}^n \frac{f(X_i)}{g(X_i)}-\E_f(h(X))\right)
\longrightarrow_{\mathcal{L}} 
$$
$$
N\left(0,\E_f\left([h(X)-\E_f(h(X))]^2 f(X)\big/g(X)\right)\right)
$$


The importance function that minimise \\
$\ds \E_f\left([h(X)-\E_f(h(X))]^2 f(X)\big/g(X)\right)$ 
is
$$
g^\#(x)=\frac{f(x)|h(x)-\E_f(h(X))|}{\int f(x)|h(x)-\E_f(h(X))|d\mu(x)}\,.
$$

\end{slide}

\section{Reminders and Additions on Markov Chains}

\begin{slide}

{\bf Definition} \\ A Markov chain is a random process $(X_k)_{k\in\mathbb{N}}$ such that
$$
\P(X_k\in A|X_0=x_0,\ldots,X_{k-1}=x_{k-1})=
$$
$$
\P(X_k\in A|X_{k-1}=x_{k-1})
$$


The Markov chain is homogenous if $\P(X_k\in A|X_{k-1}=x)$ does not depend on $k$

\end{slide}

\begin{slide}

{\bf Example: random walk}

\vs $(X_k)_{k\in\mathbb{N}}$ such that
$$
X_0\sim\nu
$$
and
$$
X_k=X_{k-1}+\epsilon_k,\quad\forall k\in\mathbb{N}^*
$$
where $\epsilon_1,\ldots$ is a random process with iid variables 
and probability distribution $\mathcal{L}$ 

\end{slide}

\begin{slide}

{\bf Definition} A (transition) kernel on $(\Omega,\mathcal{A})$ is an application 
$P:(\Omega,\mathcal{A})\longrightarrow [0,1]$ such that
\begin{itemize}
\item[1)] $\forall A\in\mathcal{A}$, $P(\cdot,A)$ is measurable
\item[2)] $\forall x\in\Omega$, $P(x,\cdot)$ is a probability distribution on 
$(\Omega,\mathcal{A})$
\end{itemize}


$(X_k)_{k\in\mathbb{N}}$ is an homogenous Markov chain with kernel $P$ if
$$
\P(X_k\in A|X_{k-1}=x)=P(x,A),\quad \forall x\in\Omega,\quad \forall A\in\mathcal{A}\,.
$$

\end{slide}

\begin{slide}

For the random walk if $\mathcal{L}=N(0,\sigma^2)$, $(X_k)_{k\in\mathbb{N}}$ 
is an homogenous Markov chain with kernel
$$
P(x,A)=\int_A \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{1}{2\sigma^2}(y-x)^2\right)dy
$$

\end{slide}

\begin{slide}

Let $(X_k)_{k\in\mathbb{N}}$ be an homogenous Markov chain with kernel $P$ 
and initial distribution $X_0\sim\nu$, we note
\begin{itemize}
\item $P_\nu$ the distribution of the chain $(X_k)_{k\in\mathbb{N}}$ 
\item $\nu P^k$ the distribution of $X_k$ : $\forall A\in\mathcal{A}$,
$$
\nu P^k(A)=\P(X_k\in A)
$$
\item $\ds P^k(x,A)=\P(X_k\in A|X_0=x)$
\end{itemize}

\end{slide}

\begin{slide}

Let $\Pi$ be a probability distribution on $(\Omega,\mathcal{A})$


We can simulate $\Pi$ in an approximate way using a homogeneous Markov chain


To do this, one must be able to build a $P$ kernel such that
for any initial $\nu$, $\nu P^k\longrightarrow_{VT} \Pi$


Total variation convergence
$$
||\nu P^k-\pi||_{VT}=\sup_{A\in\mathcal{A}}|\nu P^k(A)-\Pi(A)|
$$


Typically 
$$
\lim_{k\longrightarrow \infty} \nu P^k(A)=\Pi(A)
$$

\end{slide}

\begin{slide}

{\bf Definition}
\begin{itemize}
\vspace{-0.5cm} \item $P$ is $\Pi$-irreducible if $\forall x\in\Omega$ and $\forall A\in\mathcal{A}$
such that $\Pi(A)>0$, $\exists k(=k(x,A)$ tel que $P^k(x,A)>0$

\vs \item $P$ is $\Pi$-invariant iff $\Pi P=\Pi$ 
$$
\Pi P(A)=\int \Pi(dx_0)P(x_0,A)=\int_A \Pi(dx)\
$$ 

\vs \item $P$ is $\Pi$-reversible iff $\forall A,B \in\mathcal{A}$,
$$
\int_A P(x,B)\Pi(dx)=\int_B P(x,A)\Pi(dx)
$$
\end{itemize}

\end{slide}

\begin{slide}

If $P$ is $\Pi$-reversible then $P$ is $\Pi$-invariant


Indeed if $P$ is $\Pi$-reversible, $\forall B\in\mathcal{A}$,
$$
\int_\Omega P(x,B)\Pi(dx)=\int_B P(x,\Omega)\Pi(dx)=\int_B \Pi(dx)
$$

\end{slide}

\begin{slide}

{\bf Definition}
\begin{itemize}
\vspace{-0.25cm} \item $P$ is periodic with period $d\geq 2$ if there exists a partition 
$\Omega_1,\ldots,\Omega_d$ de $\Omega$ such that $\forall x\in\Omega_i$,
$P(x,\Omega_{i+1})=1$, $\forall i$ with the convention $d+1=1$

\vs \item A chain $\Pi$-irreducible and $\Pi$-invariant is recurrent \\
if $\forall A\in\mathcal{A}$ such that $\pi(A)>0$
\begin{enumerate}
\item[1)] $\forall x\in\Omega$, $\P(X_k\in A\,\mbox{infinitely often}|X_0=x)>0$
\item[2)] $\exists x\in \Omega$, $\P(X_k\in A\,\mbox{infinitely often}|X_0=x)=1$
\end{enumerate}

\vs \item The chain is Harris-recurrent is 2) is verified for all $x\in\Omega$

\vs \item The chain is ergodic if it is Harris-recurrent and aperiodic
\end{itemize}

\end{slide}

\section{Convergence of Markov chains}

\begin{slide}

If $P$ is $\Pi$-irreducible and $\Pi$-invariant then $P$ is recurrent


In that case, the invariant measure is unique (up to a multiplicative constant)


The chain is said to be positive recurrent if the invariant measure is a probability distribution


\end{slide}

\begin{slide}

{\bf Theorem} Suppose that $P$ is $\Pi$-irreducible et $\Pi$-invariant, then $P$ is positive recurrent
and $\Pi$ is the unique invariant distribution of $P$.
If $P$ is Harris-recurrent and aperiodic (ergodic) then
$$
\nu P^k\longrightarrow_{VT} \Pi
$$


The Harris-recurrence condition is difficult to obtain


It is satisfied for two main families of simulators:
the Gibbs sampler and the Metropolis-Hastings algorithm

\end{slide}

\begin{slide}

{\bf Theorem} If the Markov chain $(X_k)_{k\in\mathbb{N}}$ is ergodic with 
stationary distribution $\Pi$ and if $h$ is a real function
such that$\E_\Pi(|h(X)|)<\infty$, then, whatever the initial distribution $\nu$,
$$
\frac{1}{n}\sum_{i=1}^n h(X_i) \longrightarrow_{\mbox{ps}} \E_\Pi(h(X))
$$


Convergence speed?

\end{slide}

\begin{slide}

{\bf Definition} The Markov chain $(X_k)_{k\in\mathbb{N}}$ with kernel $P$
is said to be uniformly ergodic if there is $M>0 $ and $0<r<1$ such that
$$
\sup_{x\in\Omega}\sup_{A\in\mathcal{A}}|P^n(x,A)-\Pi(A)|\leq Mr^n
$$


{\bf Theorem} If the Markov chain $(X_k)_{k\in\mathbb{N}}$ is uniformly ergodic with
stationary distribution $\Pi$ and if $h$ such that $\E_\Pi(|h(X)|)<\infty$ then,
whatever the initial distribution $\nu$, there is $\sigma(h)>0$ such that
$$
\sqrt{n}\left(\frac{1}{n}\sum_{i=1}^n h(X_i)-\E_\Pi(h(X))\right)\longrightarrow_{\mathcal{L}} N(0,(\sigma(h))^2)
$$

\end{slide}

\section{The Metropolis-Hastings algorithm}

\begin{slide}

Target distribution 
$$
\Pi(dx)=\pi(x)\mu(dx)
$$


Kernel $Q$ for $x$ such that $\pi(x)>0$ 
$$
Q(x,dy)=q(x,y)\mu(dy)
$$


\end{slide}

\begin{slide}

\begin{itemize}
\item[] Choose $x^{(0)}$ such that $\pi\left(x^{(0)}\right)>0$ and set $t=1$
\item[\color{black} $(*)$] Generate $\tilde x\sim Q(x^{(t-1)},\cdot)$
\item[] If $\pi(\tilde x)=0$ then set $x^{(t)}=x^{(t-1)}$, $t=t+1$ and return to $(*)$
\item[] If $\pi(\tilde x)>0$ calculate
$$
\rho(x^{(t-1)},\tilde x)=\frac{\pi(\tilde x)\big/q(x^{(t-1)},\tilde x)}{\pi(x^{(t-1)})\big/q(\tilde x,x^{(t-1)})}
$$
\item[] Generate $u\sim\mathcal{U}([0,1])$
\item[] If $u\leq \rho(x^{(t-1)},\tilde x)$ then $x^{(t)}=\tilde x$ else $x^{(t)}=x^{(t-1)}$
\item[] set $t=t+1$ and return to $(*)$
\end{itemize}

\end{slide}

\begin{slide}

Starting from $x$ ($\pi(x)>0$), the acceptance probability of $y$ \\
($\pi(y)>0$) is given by
$$
\alpha(x,y)=\min\left[1,\frac{\pi(y)\big/q(x,y)}{\pi(x)\big/q(y,x)}\right]
$$

 
Whatever the value of $x$ such as $\pi(x)>0$,
the kernel associated with the Metropolis-Hastings algorithm is given by
$$
K(x,dy)=q(x,y)\mu(dy)\alpha(x,y)+\left[1-\int q(x,z)\alpha(x,z)\mu(dz)\right]\delta_x(dy)
$$
where $\delta_x(\cdot)$ is the Dirac mass at point $x$

\end{slide}

\begin{slide}

We can easily show that $K$ is $\Pi$-reversible

Indeed
$$
\Pi(dx)K(x,dy)=\min\left[\pi(y)q(y,x),\pi(x)q(x,y)\right]\mu(dy)\mu(dx)
$$
$$
+\left\{\pi(x)\mu(dx)-\int \min\left[\pi(z)q(z,x),\pi(x)q(x,z)\right]\mu(dz)\right\}\delta_x(dy)
$$
and
$$
\Pi(dy)K(y,dx)=\min\left[\pi(x)q(x,y),\pi(y)q(y,x)\right]\mu(dx)\mu(dy)
$$
$$
+\left\{\pi(y)\mu(dy)-\int \min\left[\pi(x)q(x,z),\pi(z)q(z,x)\right]\mu(dz)\right\}\delta_y(dx)
$$

\end{slide}

\begin{slide}

{\bf Theorem} If the kernel $Q$ is $\pi$-irreducible, the Markov chain \\
generated
with the Metropolis-Hastings algorithm is $\pi$-irreducible, $\pi$-invariant, Harris-recurrent
and aperiodic


Two particular cases \begin{itemize}
\item $Q$ is a random walk kernel: $q(x,y)=q_{RW}(x-y)$ and $q_{RW}(x)=q_{RW}(-x)$
\item $Q$ is an independent kernel: $q(x,y)=q(y)$
\end{itemize}

\end{slide}

\section{The Gibbs sampler}

\begin{slide}

Goal: generate simulations from multivariate distributions

Let $X=(X_1,X_2,\ldots,X_d)$ with probability distribution $\Pi$

Note $\Pi_i$ the conditional distribution of $X_i$ given \\
$X_{-i}=(X_1,\ldots,X_{i-1},X_{i+1},\ldots,X_d)=x_{-i}$

{\bf $\Pi_i$ is called the full conditional distribution of $X_i$}

\end{slide}

\begin{slide}

\begin{itemize}
\item[] Choose $x^{(0)}$ and set $t=1$
\item[\color{black} $(*)$] Generate $x_1^{(t)}\sim \Pi_1(\cdot|x_2^{(t-1)},\ldots,x_d^{(t-1)})$ 
\item[] Generate $x_2^{(t)}\sim \Pi_2(\cdot|x_1^{(t)},x_3^{(t-1)},\ldots,x_d^{(t-1)})$
\item[] Generate $x_3^{(t)}\sim \Pi_3(\cdot|x_1^{(t)},x_2^{(t)},x_4^{(t-1)}\ldots,x_d^{(t-1)})$
\item[] $\ldots$ 
\item[] Generate $x_d^{(t)}\sim \Pi_d(\cdot|x_1^{(t)},\ldots,x_{d-1}^{(t)})$
\item[] Set $t=t+1$ and return to $(*)$
\end{itemize}


{\bf Theorem} The Markov chain generated using the Gibbs sampler is $\Pi$-irreducible, $\Pi$-invariant, 
Harris-recurrent and aperiodic

\label{lastslide}

\end{slide}

\end{document}


